import React, { useRef, useState, useEffect, useCallback } from "react";
import Webcam from "react-webcam";
import { Geolocation } from "@capacitor/geolocation"; // ìœ„ì¹˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
import { speak } from "../src/utils/audio";
import NpuTflite from "../NpuTfliteBridge";
import { sendHazardReport } from "../src/api/report"; // â˜… ì„œë²„ ì „ì†¡ API import

// ---------------------------
// 1. Constants & Config
// ---------------------------
interface DetectedObject {
    bbox: [number, number, number, number];
    classIndex: number;
    score: number;
}

const THRESHOLD = 0.5;
const CLASS_NAMES = [
    "Stop block broken",
    "Stop block normal",
    "Straight block broken",
    "Straight block normal",
];

// TTS ë©”ì‹œì§€ ë§¤í•‘
const TTS_MESSAGES: { [key: number]: string } = {
    0: "ì „ë°©ì— íŒŒì†ëœ ì ì ë¸”ë¡ì´ ìˆìŠµë‹ˆë‹¤.",
    1: "ì „ë°©ì— ë©ˆì¶¤ ë¸”ë¡ì´ ìˆìŠµë‹ˆë‹¤.",
    2: "ì „ë°©ì— íŒŒì†ëœ ìœ ë„ ë¸”ë¡ì´ ìˆìŠµë‹ˆë‹¤.",
    3: "ì§ì§„ ìœ ë„ ë¸”ë¡ì…ë‹ˆë‹¤.",
};

const VisionCamera: React.FC = () => {
    const webcamRef = useRef<Webcam>(null);
    const canvasRef = useRef<HTMLCanvasElement>(null);

  // State
    const [modelLoaded, setModelLoaded] = useState(false);
    const [status, setStatus] = useState<string>("ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...");

  // Refs for Loop Control
    const isRunning = useRef<boolean>(true);
    const lastSpokenTime = useRef<number>(0);
  const isProcessing = useRef<boolean>(false); // ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ ë½
  const isMounted = useRef(true); // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ìƒíƒœ í™•ì¸ìš©

  // ---------------------------
  // â˜… [ì¶”ê°€ëœ ê¸°ëŠ¥] 5ì´ˆë§ˆë‹¤ ìë™ ì´¬ì˜ ë° ì—…ë¡œë“œ
  // ---------------------------
    useEffect(() => {
    isMounted.current = true;

    const autoCaptureInterval = setInterval(async () => {
      // 1. ì¹´ë©”ë¼ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìœ¼ë©´ íŒ¨ìŠ¤
        if (!webcamRef.current || !webcamRef.current.video) return;

        try {
        // 2. í˜„ì¬ ìŠ¤í¬ë¦°ìƒ· ì°ê¸° (Base64)
        const imageSrc = webcamRef.current.getScreenshot();
        if (!imageSrc) return;

        // Base64 í—¤ë” ì œê±° ('data:image/jpeg;base64,' ë¶€ë¶„ ìë¥´ê¸°)
        const base64Data = imageSrc.split(",")[1];

        // 3. í˜„ì¬ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
        // (ì£¼ì˜: ì‹¤ë‚´ì—ì„œëŠ” GPSê°€ ì•ˆ ì¡í ìˆ˜ ìˆìœ¼ë‹ˆ íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ì²œ)
        const pos = await Geolocation.getCurrentPosition({ timeout: 5000 });

        console.log("ğŸ“¸ 5ì´ˆ ìë™ ì´¬ì˜: ì„œë²„ ì „ì†¡ ì‹œë„...");

        // 4. ì„œë²„ë¡œ ì „ì†¡ (ìœ„í—˜ë„ëŠ” ì¼ë‹¨ 0ìœ¼ë¡œ ê³ ì •)
        await sendHazardReport({
            latitude: pos.coords.latitude,
            longitude: pos.coords.longitude,
          hazard_type: "Auto_Capture", // ìë™ ê¸°ë¡ìš© íƒœê·¸
          risk_level: 0, // ìœ„í—˜ë„ 0
            imageBase64: base64Data,
            description: "5ì´ˆ ì£¼ê¸° ìë™ ê¸°ë¡ ë°ì´í„°",
        });

        console.log("âœ… ì„œë²„ ì—…ë¡œë“œ ì„±ê³µ");
        } catch (error) {
        console.error("âŒ ìë™ ì—…ë¡œë“œ ì‹¤íŒ¨:", error);
        }
    }, 5000); // 5000ms = 5ì´ˆ

    // ì»´í¬ë„ŒíŠ¸ê°€ êº¼ì§ˆ ë•Œ íƒ€ì´ë¨¸ ì •ë¦¬
    return () => {
        isMounted.current = false;
        clearInterval(autoCaptureInterval);
    };
    }, []);

  // ---------------------------
  // 2. Load Model
  // ---------------------------
    useEffect(() => {
    const loadModel = async () => {
        try {
        setStatus("AI ëª¨ë¸ ë¡œë”© ì¤‘...");
        // ëª¨ë¸ ê²½ë¡œ (public í´ë” ê¸°ì¤€)
        const res = await NpuTflite.loadModel({
            modelPath: "best_float16.tflite",
        });
        setStatus(`ì¤€ë¹„ ì™„ë£Œ (${res.delegate})`);
        setModelLoaded(true);
        } catch (err: any) {
        setStatus(`ëª¨ë¸ ì˜¤ë¥˜: ${err.message}`);
        console.error("Model Load Error:", err);
        }
    };
    loadModel();

    return () => {
        isRunning.current = false;
    };
    }, []);

  // ---------------------------
  // 3. Post-Processing & TTS
  // ---------------------------
    const processOutput = (
    outputTensor: any,
    imgWidth: number,
    imgHeight: number,
    ): DetectedObject[] => {
    const shape = outputTensor.shape;
    const data = outputTensor.dataSync() as Float32Array;

    if (!shape || shape.length < 3) return [];

    const dim1 = shape[1];
    const dim2 = shape[2];
    const isTransposed = dim1 < dim2;

    const numChannels = isTransposed ? dim1 : dim2;
    const numAnchors = isTransposed ? dim2 : dim1;
    const scoreStartIdx = 4;

    const detections: DetectedObject[] = [];

    for (let i = 0; i < numAnchors; i++) {
        let maxScore = 0;
        let classIndex = -1;

        for (let c = 0; c < CLASS_NAMES.length; c++) {
        let val = 0;
        if (isTransposed) {
          val = data[(scoreStartIdx + c) * numAnchors + i];
        } else {
          val = data[i * numChannels + (scoreStartIdx + c)];
        }

        if (val > maxScore) {
            maxScore = val;
            classIndex = c;
        }
        }

        if (maxScore > THRESHOLD) {
        let cx, cy, w, h;
        if (isTransposed) {
          cx = data[0 * numAnchors + i];
          cy = data[1 * numAnchors + i];
          w = data[2 * numAnchors + i];
          h = data[3 * numAnchors + i];
        } else {
          cx = data[i * numChannels + 0];
          cy = data[i * numChannels + 1];
          w = data[i * numChannels + 2];
          h = data[i * numChannels + 3];
        }

        detections.push({
            bbox: [
            (cx - w / 2) * imgWidth,
            (cy - h / 2) * imgHeight,
            w * imgWidth,
            h * imgHeight,
            ],
            classIndex,
            score: maxScore,
        });
        }
    }

    detections.sort((a, b) => b.score - a.score);
    return detections.slice(0, 5);
    };

    const handleTTS = (detections: DetectedObject[]) => {
    if (detections.length === 0) return;

    const topDet = detections[0];
    const now = Date.now();

    if (now - lastSpokenTime.current > 3000) {
        const msg = TTS_MESSAGES[topDet.classIndex];
        if (msg) {
        speak(msg);
        lastSpokenTime.current = now;
        }
    }
    };

    const drawResults = (
    detections: DetectedObject[],
    videoWidth: number,
    videoHeight: number,
    ) => {
    const ctx = canvasRef.current?.getContext("2d");
    if (!ctx || !canvasRef.current) return;

    if (
        canvasRef.current.width !== videoWidth ||
        canvasRef.current.height !== videoHeight
    ) {
        canvasRef.current.width = videoWidth;
        canvasRef.current.height = videoHeight;
    }

    ctx.clearRect(0, 0, videoWidth, videoHeight);

    detections.forEach((det) => {
        const [x, y, w, h] = det.bbox;
      const label = `${CLASS_NAMES[det.classIndex]} ${Math.round(det.score * 100)}%`;

        ctx.strokeStyle = "#00FF00";
        ctx.lineWidth = 4;
        ctx.strokeRect(x, y, w, h);

        ctx.fillStyle = "#00FF00";
        const textWidth = ctx.measureText(label).width;
        ctx.fillRect(x, y - 25, textWidth + 10, 25);

        ctx.fillStyle = "#000000";
        ctx.font = "bold 16px Arial";
        ctx.fillText(label, x + 5, y - 5);
    });
    };

  // ---------------------------
  // 4. Real-time Inference Loop
  // ---------------------------
    const detectFrame = useCallback(async () => {
    if (!isRunning.current || !modelLoaded || !webcamRef.current?.video) return;
    if (isProcessing.current) return;

    isProcessing.current = true;

    try {
        const video = webcamRef.current.video;
        if (video.readyState !== 4) {
        isProcessing.current = false;
        setTimeout(detectFrame, 100);
        return;
        }

        const imageSrc = webcamRef.current.getScreenshot();

        if (imageSrc) {
        const base64Data = imageSrc.split(",")[1];
        const result = await NpuTflite.detect({ image: base64Data });

        if (result && result.data) {
            const outputTensor = {
            dataSync: () => Float32Array.from(result.data),
            shape: result.shape,
            };

            const vWidth = video.videoWidth;
            const vHeight = video.videoHeight;

            const detections = processOutput(outputTensor, vWidth, vHeight);
            drawResults(detections, vWidth, vHeight);
            handleTTS(detections);
        }
        }
    } catch (error: any) {
        console.error("Predict Error:", error);
        } finally {
        isProcessing.current = false;
        if (isRunning.current) {
        setTimeout(detectFrame, 50);
        }
    }
    }, [modelLoaded]);

    useEffect(() => {
        if (modelLoaded) {
        detectFrame();
    }
    }, [modelLoaded, detectFrame]);

    return (
    <div className="relative w-full h-full bg-black flex justify-center items-center overflow-hidden">
        <Webcam
        ref={webcamRef}
        audio={false}
        screenshotFormat="image/jpeg"
        videoConstraints={{ facingMode: "environment" }}
        style={{
            position: "absolute",
            width: "100%",
            height: "100%",
            objectFit: "contain",
        }}
        />

        <canvas
        ref={canvasRef}
        style={{
            position: "absolute",
            width: "100%",
            height: "100%",
            objectFit: "contain",
        }}
        />

      {/* ìƒíƒœ í‘œì‹œ ì˜¤ë²„ë ˆì´ */}
        <div className="absolute top-4 right-4 bg-black/60 px-3 py-1 rounded-full z-50">
        <p className="text-yellow-400 font-mono text-xs font-bold animate-pulse">
            {status}
        </p>
        </div>
    </div>
    );
};

export default VisionCamera;
