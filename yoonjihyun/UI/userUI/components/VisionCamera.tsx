import React, { useRef, useState, useEffect } from "react";
import Webcam from "react-webcam";
import { Geolocation } from "@capacitor/geolocation";
import { sendHazardReport } from "../src/api/report";
import NpuTflite from "../NpuTfliteBridge";
import { YoloParser, DetectedBox } from "../src/utils/YoloParser";

const MODEL_PATH = "wasm/yolo26n_float32.tflite";

const VisionCamera: React.FC = () => {
  const webcamRef = useRef<Webcam>(null);

  const [status, setStatus] = useState<string>("ëª¨ë¸ ë¡œë”© ì¤‘...");
  const [inferenceInfo, setInferenceInfo] = useState<string>("");

  const isMounted = useRef(true);
  const [modelLoaded, setModelLoaded] = useState(false);

  // âœ… setInterval + async ì¤‘ì²© ì‹¤í–‰ ë°©ì§€
  const isRunningRef = useRef(false);

  // 1) ëª¨ë¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ 1íšŒ)
  useEffect(() => {
    isMounted.current = true;

    const loadModel = async () => {
      try {
        console.log("ğŸ› ï¸ YOLO26n TFLite ëª¨ë¸ ë¡œë“œ ì‹œë„...", MODEL_PATH);
        const result = await NpuTflite.loadModel({ modelPath: MODEL_PATH });
        console.log("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ:", result);

        if (!isMounted.current) return;
        setStatus("ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ");
        setModelLoaded(true);
      } catch (error) {
        console.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", error);
        if (!isMounted.current) return;
        setStatus("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨");
        setModelLoaded(false);
      }
    };

    loadModel();

    return () => {
      isMounted.current = false;
    };
  }, []);

  /**
   * âœ… YOLO TFLite ì¶œë ¥(data/shape)ì´ í”Œë«í¼/ë¸Œë¦¿ì§€ë³„ë¡œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ ì •ê·œí™”
   * - ëŒ€í‘œ í˜•íƒœ:
   *   (1, 84, 8400) / (1, 85, 8400)
   *   (8400, 84) / (8400, 85)
   *   shape ëˆ„ë½ + data ê¸¸ì´ë¡œ ì¶”ì •
   */
  const normalizeYoloOutput = (
    data: any,
    shape: any
  ): { flat: number[]; normShape: number[] } => {
    // data -> number[]
    let flat: number[] = [];
    if (Array.isArray(data)) {
      flat = data.map((v) => Number(v));
    } else if (data && typeof data.length === "number") {
      flat = Array.from(data as ArrayLike<number>, (v) => Number(v));
    } else {
      flat = [];
    }

    // shape -> number[]
    let normShape: number[] = [];
    if (Array.isArray(shape) && shape.length) {
      normShape = shape.map((v) => Number(v));
    }

    // shape ì—†ìœ¼ë©´ data ê¸¸ì´ë¡œ ì¶”ì •
    if (normShape.length === 0 && flat.length > 0) {
      // 640 ê¸°ì¤€ 8400 boxesê°€ ê°€ì¥ í”í•¨ (P3/P4/P5 í•©)
      // ë‹¤ë¥¸ ì…ë ¥(320/1280)ë„ ë°©ì–´ì ìœ¼ë¡œ í¬í•¨
      const boxCandidates = [8400, 2100, 33600];

      for (const n of boxCandidates) {
        if (flat.length % n === 0) {
          const attrs = flat.length / n; // 84, 85, 9 ë“±
          normShape = [1, attrs, n];
          break;
        }
      }
    }

    // (boxes, attrs) í˜•íƒœë©´ (1, attrs, boxes)ë¡œ í‘œì¤€í™”
    if (normShape.length === 2) {
      const a = normShape[0];
      const b = normShape[1];

      // attrs ë²”ìœ„(ëŒ€ëµ 6~300)ë¥¼ ì´ìš©í•´ íŒë³„
      if (b >= 6 && b <= 300) {
        // (boxes, attrs)
        normShape = [1, b, a];
      } else if (a >= 6 && a <= 300) {
        // (attrs, boxes) -> ì´ë¯¸ ê´œì°®ì§€ë§Œ í‘œì¤€ìœ¼ë¡œ ë§ì¶¤
        normShape = [1, a, b];
      }
    }

    return { flat, normShape };
  };

  // 2) í†µí•© ë£¨í”„: 3ì´ˆë§ˆë‹¤ ì´¬ì˜ -> ì¶”ë¡  -> ì „ì†¡
  useEffect(() => {
    if (!modelLoaded) return;

    const loopInterval = setInterval(async () => {
      if (!isMounted.current || !webcamRef.current) return;
      if (isRunningRef.current) return; // âœ… ì¤‘ì²© ë°©ì§€
      isRunningRef.current = true;

      try {
        const imageSrc = webcamRef.current.getScreenshot();
        if (!imageSrc) return;

        // [Step 1] Letterbox Preprocessing (640x640) - âœ… ë³€ê²½í•˜ì§€ ì•ŠìŒ
        const img = new Image();
        img.src = imageSrc;

        await new Promise<void>((resolve, reject) => {
          img.onload = () => resolve();
          img.onerror = () => reject(new Error("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨"));
        });

        const modelInputSize = 640;
        const canvas = document.createElement("canvas");
        canvas.width = modelInputSize;
        canvas.height = modelInputSize;

        const ctx = canvas.getContext("2d");
        if (!ctx) return;

        // Fill with black (padding)
        ctx.fillStyle = "black";
        ctx.fillRect(0, 0, modelInputSize, modelInputSize);

        // Resize with aspect ratio preserved
        const scale = Math.min(modelInputSize / img.width, modelInputSize / img.height);
        const w = img.width * scale;
        const h = img.height * scale;
        const tx = (modelInputSize - w) / 2;
        const ty = (modelInputSize - h) / 2;

        // Draw image centered (letterboxed)
        ctx.drawImage(img, tx, ty, w, h);

        // Inferenceìš© Base64 extraction (Letterboxed Image)
        const letterboxedBase64 = canvas.toDataURL("image/jpeg", 0.9).split(",")[1];

        // [Step 2] NPU ì¶”ë¡  ì‹¤í–‰
        const startTime = performance.now();
        const result = await NpuTflite.detect({ image: letterboxedBase64 });
        const endTime = performance.now();
        const duration = (endTime - startTime).toFixed(0);

        // [Step 3] ê²°ê³¼ íŒŒì‹± ë° ê·¸ë¦¬ê¸° (same canvas)
        let infoMsg = `ì‹œê°„: ${duration}ms`;

        if (result && result.data && (result.data.length ?? 0) > 0) {
          infoMsg += ` | ë°ì´í„°: ${result.data.length}ê°œ`;
          if (result.shape) infoMsg += ` | Shape: [${result.shape.join("x")}]`;

          const { flat, normShape } = normalizeYoloOutput(result.data, result.shape);

          // âœ… íŒŒì„œ ì…ë ¥ ì•ˆì •í™”
          const boxes: DetectedBox[] = YoloParser.parse(flat, normShape);

          if (boxes.length > 0) {
            infoMsg += ` | ğŸ“¦ê°ì²´: ${boxes.length}ê°œ`;

            ctx.lineWidth = 2;
            ctx.font = "bold 20px Arial";

            boxes.forEach((box) => {
              // YOLO ê²°ê³¼ëŠ” 0~1 ì •ê·œí™”ëœ ì¢Œí‘œ ê°€ì •
              const x = box.x * modelInputSize;
              const y = box.y * modelInputSize;
              const width = box.w * modelInputSize;
              const height = box.h * modelInputSize;

              const left = x - width / 2;
              const top = y - height / 2;

              // ë°•ìŠ¤
              ctx.strokeStyle = "#00FF00";
              ctx.strokeRect(left, top, width, height);

              // ë¼ë²¨
              const labelText = `${box.className} ${(box.score * 100).toFixed(0)}%`;
              const textWidth = ctx.measureText(labelText).width;

              ctx.fillStyle = "#00FF00";
              ctx.fillRect(left, top - 25, textWidth + 10, 25);

              ctx.fillStyle = "black";
              ctx.fillText(labelText, left + 5, top - 5);
            });
          } else {
            infoMsg += " | âšªê°ì²´ì—†ìŒ";
          }
        } else {
          infoMsg += " | ì¶œë ¥ì—†ìŒ";
        }

        setInferenceInfo(infoMsg);
        console.log(`ğŸ” ì¶”ë¡  ì™„ë£Œ: ${infoMsg}`);

        // [Step 4] ë¦¬í¬íŠ¸ ì „ì†¡ (Letterboxed + Boxes Image)
        const finalImageBase64 = canvas.toDataURL("image/jpeg", 0.7).split(",")[1];
        const position = await Geolocation.getCurrentPosition();

        console.log("ğŸ“¤ 3ì´ˆ ì£¼ê¸° ë°ì´í„° ì „ì†¡ ì¤‘...");
        await sendHazardReport({
          latitude: position.coords.latitude,
          longitude: position.coords.longitude,
          hazard_type: "Periodic_Monitor",
          risk_level: 1,
          description: `ëª¨ë‹ˆí„°ë§ (3ì´ˆ ì£¼ê¸°) ${new Date().toLocaleTimeString()} / ${infoMsg}`,
          imageBase64: finalImageBase64,
        });

        setStatus("ì „ì†¡ ì™„ë£Œ");
        setTimeout(() => {
          if (isMounted.current) setStatus("ëª¨ë‹ˆí„°ë§ ì¤‘...");
        }, 1000);
      } catch (error) {
        console.error("ë£¨í”„ ì—ëŸ¬:", error);
        setStatus("ì—ëŸ¬ ë°œìƒ");
        setInferenceInfo(`ì—ëŸ¬: ${String(error)}`);
      } finally {
        isRunningRef.current = false;
      }
    }, 3000);

    return () => {
      isMounted.current = false;
      clearInterval(loopInterval);
    };
  }, [modelLoaded]);

  return (
    <div className="relative w-full h-full bg-black flex justify-center items-center overflow-hidden">
      <Webcam
        ref={webcamRef}
        audio={false}
        screenshotFormat="image/jpeg"
        videoConstraints={{ facingMode: "environment" }}
        style={{
          position: "absolute",
          width: "100%",
          height: "100%",
          objectFit: "cover",
        }}
      />

      {/* ìƒíƒœ í‘œì‹œ */}
      <div className="absolute top-4 right-4 flex flex-col items-end gap-2 z-50">
        <div className="bg-black/60 px-3 py-1 rounded-full">
          <p className="text-yellow-400 font-mono text-xs font-bold animate-pulse">
            {status}
          </p>
        </div>

        {inferenceInfo && (
          <div className="bg-blue-900/80 px-3 py-1 rounded-lg border border-blue-400">
            <p className="text-white font-mono text-[10px] whitespace-pre-wrap max-w-[200px]">
              {inferenceInfo}
            </p>
          </div>
        )}
      </div>
    </div>
  );
};

export default VisionCamera;
